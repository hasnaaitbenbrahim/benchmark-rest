# R√©sum√© Concret du Projet Benchmark REST

## üìã Vue d'ensemble du projet

Ce projet est une **√©tude de performance comparative** qui √©value 3 impl√©mentations diff√©rentes d'API REST Java pour exposer des donn√©es relationnelles (Cat√©gories et Items). L'objectif est de comparer les performances, la consommation de ressources et la facilit√© d'utilisation de chaque approche.

---

## üéØ Objectif

D√©terminer quelle variante d'API REST est la plus performante selon diff√©rents crit√®res :
- **D√©bit** (RPS - Requests Per Second)
- **Latence** (p50, p95, p99)
- **Consommation de ressources** (CPU, m√©moire, threads)
- **Facilit√© d'exposition relationnelle**
- **Gestion des JOINs et requ√™tes N+1**

---

## üèóÔ∏è Architecture du projet

### Les 3 variantes compar√©es

#### **Variante A : Jersey** (`variantA-jersey/`)
- **Framework** : Jakarta EE / Jersey (JAX-RS)
- **ORM** : JPA (Jakarta Persistence)
- **Caract√©ristiques** :
  - Contr√¥leurs manuels avec annotations `@Path`, `@GET`, `@POST`, etc.
  - Gestion explicite des EntityManager
  - Pagination manuelle
  - Endpoints : `/categories`, `/items`, `/categories/{id}/items`
- **Port** : 8081

#### **Variante C : Spring MVC** (`variantC-springmvc/`)
- **Framework** : Spring Boot avec `@RestController`
- **ORM** : Spring Data JPA
- **Caract√©ristiques** :
  - Contr√¥leurs avec annotations Spring (`@RestController`, `@GetMapping`, etc.)
  - Repository pattern avec Spring Data JPA
  - Pagination Spring (Pageable)
  - Endpoints identiques √† la variante A
- **Port** : 8082

#### **Variante D : Spring Data REST** (`variantD-springdatarest/`)
- **Framework** : Spring Boot avec Spring Data REST
- **ORM** : Spring Data JPA
- **Caract√©ristiques** :
  - **Exposition automatique** des repositories
  - Format HAL (Hypertext Application Language)
  - Pas de contr√¥leurs manuels
  - Endpoints HAL standard : `/categories`, `/items`, `/categories/{id}/items`
- **Port** : 8083

### Infrastructure de test

#### Base de donn√©es
- **PostgreSQL 16** (Docker)
- Tables : `category`, `item` (relation 1-N)
- Donn√©es de test pr√©-charg√©es

#### Outils de monitoring
- **Prometheus** : Collecte des m√©triques JVM (CPU, m√©moire, GC, threads, HikariCP)
- **Grafana** : Visualisation des dashboards
- **InfluxDB 2** : Stockage des m√©triques JMeter

#### Outils de test de charge
- **JMeter** : Ex√©cution des sc√©narios de test
- 4 sc√©narios d√©finis dans des fichiers `.jmx`

---

## üß™ M√©thodologie : Comment le benchmark a √©t√© r√©alis√©

### 1. Sc√©narios de test (4 sc√©narios)

#### **Sc√©nario 1 : READ-heavy**
- **Description** : Charge de lecture intensive
- **Mix de requ√™tes** :
  - 50% : GET `/items?page=X&size=50` (liste pagin√©e)
  - 20% : GET `/items?categoryId=X` (filtrage par cat√©gorie)
  - 20% : GET `/categories/{id}/items` (items d'une cat√©gorie)
  - 10% : GET `/categories?page=X&size=50` (liste de cat√©gories)
- **Param√®tres** :
  - Threads : 50 ‚Üí 100 ‚Üí 200 (paliers progressifs)
  - Ramp-up : 60 secondes
  - Dur√©e : 10 minutes par palier
- **Objectif** : Tester les performances en lecture, notamment les JOINs

#### **Sc√©nario 2 : JOIN-filter**
- **Description** : Filtrage avec jointures (cas critique)
- **Mix de requ√™tes** :
  - 70% : GET `/items?categoryId=X` (requ√™te avec JOIN)
  - 30% : GET `/items/{id}` (requ√™te simple)
- **Param√®tres** :
  - Threads : 60 ‚Üí 120
  - Ramp-up : 60 secondes
  - Dur√©e : 8 minutes par palier
- **Objectif** : Mesurer l'impact des requ√™tes avec JOIN et d√©tecter les probl√®mes N+1

#### **Sc√©nario 3 : MIXED**
- **Description** : Mix lecture/√©criture (CRUD complet)
- **Mix de requ√™tes** :
  - GET `/items`, GET `/categories`
  - POST `/items`, POST `/categories`
  - PUT `/items/{id}`, PUT `/categories/{id}`
  - DELETE `/items/{id}`, DELETE `/categories/{id}`
- **Param√®tres** :
  - Threads : 50 ‚Üí 100
  - Ramp-up : 60 secondes
  - Dur√©e : 10 minutes par palier
  - Payload : 1 KB (fichier `payloads_1k.csv`)
- **Objectif** : Tester les performances en √©criture et transactions

#### **Sc√©nario 4 : HEAVY-body**
- **Description** : Payloads lourds (simulation de donn√©es complexes)
- **Mix de requ√™tes** :
  - POST `/items` avec payload de 5 KB
  - PUT `/items/{id}` avec payload de 5 KB
- **Param√®tres** :
  - Threads : 30 ‚Üí 60
  - Ramp-up : 60 secondes
  - Dur√©e : 8 minutes par palier
  - Payload : 5 KB (fichier `payloads_5k.csv`)
- **Objectif** : Tester la gestion des gros payloads et la s√©rialisation

### 2. Donn√©es de test

#### Fichiers CSV g√©n√©r√©s
- **`ids.csv`** : 1000 paires (itemId, categoryId) pour varier les requ√™tes
- **`payloads_1k.csv`** : 50 payloads JSON de ~1 KB
- **`payloads_5k.csv`** : 20 payloads JSON de ~5 KB

### 3. Processus d'ex√©cution

Le script `run_benchmark.sh` automatise tout le processus :

```bash
1. Compilation des 3 variantes (Maven)
2. D√©marrage de l'infrastructure Docker :
   - PostgreSQL (base de donn√©es)
   - InfluxDB (m√©triques JMeter)
   - Prometheus (m√©triques JVM)
   - Grafana (visualisation)
3. D√©marrage des 3 services REST (ports 8081, 8082, 8083)
4. Pour chaque variante :
   - Ex√©cution des 4 sc√©narios JMeter
   - Collecte des r√©sultats dans results/{variante}/
5. G√©n√©ration du r√©sum√© (results/summary.md)
```

### 4. M√©triques collect√©es

#### M√©triques JMeter (par requ√™te)
- **Samples** : Nombre total de requ√™tes
- **RPS** : Requests Per Second (d√©bit)
- **Latence** : p50, p95, p99 (percentiles)
- **Taux d'erreurs** : Pourcentage de requ√™tes √©chou√©es
- **Temps de r√©ponse** : Temps moyen, min, max

#### M√©triques JVM (via Prometheus)
- **CPU** : Utilisation processeur (%)
- **Heap** : M√©moire heap utilis√©e (MB)
- **GC** : Temps de garbage collection (ms/s)
- **Threads** : Nombre de threads actifs
- **HikariCP** : Pool de connexions (actives/max)

---

## üìä R√©sultats

### √âtat actuel

D'apr√®s l'analyse du projet, **les benchmarks ont √©t√© ex√©cut√©s** mais les r√©sultats d√©taill√©s ne sont pas encore compl√®tement analys√©s dans le fichier `results/summary.md` (fichier vide actuellement).

### Structure des r√©sultats

Les r√©sultats sont organis√©s comme suit :

```
results/
‚îú‚îÄ‚îÄ summary.md                    # R√©sum√© global (√† g√©n√©rer)
‚îú‚îÄ‚îÄ varianta/                     # R√©sultats variante A (Jersey)
‚îÇ   ‚îú‚îÄ‚îÄ read-heavy.jtl          # R√©sultats d√©taill√©s
‚îÇ   ‚îú‚îÄ‚îÄ join-filter.jtl
‚îÇ   ‚îú‚îÄ‚îÄ mixed.jtl
‚îÇ   ‚îî‚îÄ‚îÄ heavy-body.jtl
‚îú‚îÄ‚îÄ variantc/                     # R√©sultats variante C (Spring MVC)
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ variantd/                     # R√©sultats variante D (Spring Data REST)
    ‚îî‚îÄ‚îÄ ...
```

### Format des r√©sultats attendus

Le script g√©n√®re automatiquement un tableau comparatif :

| Service | Scenario | Samples | RPS | p50(ms) | p95(ms) | p99(ms) | Errors(%) |
|---------|----------|---------|-----|---------|---------|---------|-----------|
| varianta | read-heavy | ... | ... | ... | ... | ... | ... |
| variantc | read-heavy | ... | ... | ... | ... | ... | ... |
| variantd | read-heavy | ... | ... | ... | ... | ... | ... |

---

## üìà Tableaux d'analyse (T0-T7)

Le projet pr√©voit 8 tableaux d'analyse d√©taill√©s :

### **T0** : Configuration mat√©rielle & logicielle
- Machine (CPU, RAM)
- Versions (Java, Docker, PostgreSQL, JMeter, etc.)
- Param√®tres JVM (Xms, Xmx, GC)
- Configuration HikariCP

### **T1** : Sc√©narios
- D√©finition des 4 sc√©narios de test

### **T2** : R√©sultats JMeter
- Comparaison RPS, latence, erreurs pour chaque variante

### **T3** : Ressources JVM
- CPU, m√©moire, GC, threads, HikariCP par variante

### **T4** : D√©tails par endpoint (JOIN-filter)
- Performance d√©taill√©e des endpoints avec JOIN

### **T5** : D√©tails par endpoint (MIXED)
- Performance d√©taill√©e des op√©rations CRUD

### **T6** : Incidents / erreurs
- Analyse des erreurs rencontr√©es

### **T7** : Synth√®se & conclusion
- Comparaison globale
- Recommandations d'usage

---

## üîß Comment ex√©cuter le benchmark

### M√©thode automatique (recommand√©e)

```bash
# 1. Rendre le script ex√©cutable
chmod +x run_benchmark.sh

# 2. Lancer le benchmark complet
./run_benchmark.sh
```

**Dur√©e estim√©e** : 30-60 minutes

### M√©thode manuelle

Voir le fichier `GUIDE_EXECUTION.md` pour les √©tapes d√©taill√©es.

---

## üì¶ Livrables du projet

Le projet comprend 5 livrables principaux :

1. **‚úÖ Livrable 1** : Code des 3 variantes (A, C, D) - **COMPLET**
2. **‚úÖ Livrable 2** : Fichiers JMeter (.jmx) + CSV - **COMPLET**
3. **‚ö†Ô∏è Livrable 3** : Dashboards Grafana + Exports - **√Ä cr√©er**
4. **‚ö†Ô∏è Livrable 4** : Tableaux T0-T7 + Analyse - **√Ä remplir**
5. **‚ö†Ô∏è Livrable 5** : Recommandations d'usage - **√Ä r√©diger**

---

## üéì Conclusions attendues

Apr√®s analyse compl√®te, le projet doit permettre de r√©pondre √† :

1. **Quelle variante est la plus performante pour les lectures relationnelles ?**
   - Impact des JOINs
   - Probl√®mes N+1 queries
   - Performance de la pagination

2. **Quelle variante est la plus performante pour l'√©criture ?**
   - Gestion des transactions
   - Performance POST/PUT

3. **Quelle variante est la plus facile √† exposer rapidement ?**
   - Temps de d√©veloppement
   - Facilit√© de maintenance
   - Trade-off performance vs facilit√©

---

## üîç Points techniques importants

### Diff√©rences cl√©s entre les variantes

| Aspect | Variante A (Jersey) | Variante C (Spring MVC) | Variante D (Spring Data REST) |
|--------|---------------------|-------------------------|-------------------------------|
| **Contr√¥leurs** | Manuels (`@Path`) | Manuels (`@RestController`) | Automatiques (repositories) |
| **Format r√©ponse** | JSON standard | JSON standard | HAL (Hypertext Application Language) |
| **Pagination** | Manuelle | Spring Pageable | Spring Pageable (automatique) |
| **JOINs** | Gestion explicite | Gestion via JPA | Gestion via JPA (automatique) |
| **Complexit√©** | Moyenne | Faible | Tr√®s faible |
| **Contr√¥le** | Total | √âlev√© | Limit√© |

### Probl√®mes potentiels √† analyser

1. **N+1 queries** : Chargement lazy des relations
2. **JOIN FETCH** : Optimisation des requ√™tes relationnelles
3. **Pagination** : Impact sur les performances
4. **Format HAL** : Overhead de s√©rialisation (variante D)
5. **Pool de connexions** : Configuration HikariCP

---

## üìù Notes importantes

- Les 3 variantes utilisent la **m√™me base de donn√©es** PostgreSQL
- Les tests sont ex√©cut√©s **s√©quentiellement** (une variante √† la fois) pour des mesures pr√©cises
- Les m√©triques sont collect√©es en **temps r√©el** via Prometheus et InfluxDB
- Les r√©sultats sont **reproductibles** gr√¢ce √† la configuration Docker

---

## üöÄ Prochaines √©tapes

1. **Ex√©cuter le benchmark** si ce n'est pas d√©j√† fait : `./run_benchmark.sh`
2. **Analyser les r√©sultats** dans `results/summary.md`
3. **Cr√©er les dashboards Grafana** pour visualisation
4. **Remplir les tableaux T0-T7** avec les donn√©es collect√©es
5. **R√©diger les recommandations** d'usage selon les r√©sultats

---

## üìö Documentation suppl√©mentaire

- `README.md` : Vue d'ensemble
- `GUIDE_EXECUTION.md` : Guide d'ex√©cution d√©taill√©
- `LIVRABLES.md` : Description des livrables
- `tableaux_T0_T7.md` : Template des tableaux d'analyse
- `VERIFICATION_LIVRABLE2.md` : V√©rification des fichiers JMeter

---

**Date de cr√©ation** : Analyse du projet benchmark-rest  
**Auteur** : R√©sum√© g√©n√©r√© √† partir de l'analyse du code

